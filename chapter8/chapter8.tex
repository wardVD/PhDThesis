\chapter{The SPACE Analysis}
After introducing the detector workings, reconstruction and analysis techniques, background contributions and the signature of the signal, this chapter gives an overview of analysis. Starting from data processed with basic reconstructions and requirements a workflow was set up to try to discriminate events that are most likely of known physical interactions from the rare events that are sought for in this analysis. These events would originate from the theoretical particles with an anomalous charge (see Chapter \ref{ch:theoreticalmotivation}). The analysis was adopted the "SPACE" analysis, which stands for a "Search for Particle with Anomalous ChargE".

\section{Filter selection}
As explained in Section \ref{subsec:filters}, the data is processed through multiple filters. Since this analysis is the first of its kind in the collaboration, no processed dataset from other analyses was used. Filters had to be selected for proper comparison of data and Monte Carlo and I have chosen to optimize the signal to background ratio to select which filters should be included. An illustration is given in Fig. \ref{fig:filterrate}. This filter selection will be referred to as \textit{Level2b}, as a simple addition to filter processing in Level2 (see Section \ref{sec:processing}). 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{chapter8/img/FilterRate.png}
\caption{Illustration of the efficiencies of several filters and their possible combinations. The x-axis was determined by starting with filter selections that had a low efficiency in signal selection and range in function of performance. Five signal points for a fixed charge and different mass show similar results. Exotic SMPs with charges 1/2 and 2/3 show very similar results but are left out for a better visualization.}
\label{fig:filterrate}
\end{figure}

\subsection{VEF}
The Vertical Event Filter (VEF) is designed to be used for oscillation and Earth WIMP analyses and makes use of the string trigger (see Section \ref{subsec:triggers}). An SMP that travel alongside a string, or closeby, can trigger optical modules while the total light yield of an event is low, making this filter an ideal addition to the filters that are selected. In addition, the filter removes HLC hits in the top 5 DOM layers to reduce the muonic component from air shower events. Other selection cuts, try to optimize the search efficiency for WIMP events in particular. For example, the LF zenith angle should be higher than 68.7$^\circ$. More information can be found in Ref. \cite{VEF2012}.

%meeste hiervan: https://docushare.icecube.wisc.edu/dsweb/Get/Document-62750/VEF_2013_proposal.pdf

\subsection{LowUp}
The LowUp filter is again mainly designed for WIMP searches, but also atmospheric neutrino analysis and is mainly designed to capture up-going muons with an energy below 1 TeV. The majority of the events that are selected by this filter make use of the in-ice Volume Trigger (see Table \ref{tab:trigger}), but also the in-ice SMT8, in-ice String and SMT3-DeepCore triggers are run over for completeness. The selection cuts are loose selections required to look for up-going track-like particles. For example, the zenith angle of the reconstructed particle should have an angle of 80$^\circ$ or higher and the difference between the maximal z-coordinate and minimal z-coordinate of hit DOMs should be less than or equal to 600 m. More information can be found in Ref. \cite{LowUp2012}.

%je hebt een Nchan>=5 cut in je L4 zodat veranderingen in LowUp niet uitmaken :-)

\subsection{Online Muon L2}
The Online Muon L2 filter is a subset of the Muon Filter (see Ref. \ref{Muon2012}) and tries to select the most interesting muon-like events while reducing the rate of the filter from around 30 Hz to 5 Hz, reducing the data with a factor of 6. Historically this subset was processed data from the Muon Filter, but after realizing that this could be done online and because many analyses made use of this selection, it was chosen to implement it as a separate filter. The filter tries to select both up-going and down-going muons, with different selection cuts depending on the zenith angle of the particle reconstruction. The four selection ranges are defined as:
\vspace{2mm}
\begin{itemize}
\item $180^\circ \geq \theta_\textrm{MPE} \geq 115^\circ$
\item $115^\circ > \theta_\textrm{MPE} \geq 82^\circ$
\item $82^\circ > \theta_\textrm{MPE} \geq 66^\circ$
\item $66^\circ > \theta_\textrm{MPE} \geq 0^\circ$
\end{itemize}
where the particle reconstruction was done with MPE (Section \ref{subsec:spempe}), which was feasible if it only had to be done on the events passing the Muon Filter. The first two regions have an efficiency\footnote{Here defined as having a reconstruction within 3$^\circ$ of the MC truth.} higher than 99\%. The down-going region require more stringent cuts to remove the less interesting muons from air showers. The variables used are the number ot hit DOMs, likelihood parameters, number of PEs and so on. More information can be found in Ref. \cite{OnlineMuonL22012}.

\textcolor{red}{Verhoogt uw signaal niet zo veel omdat je enkel upgoing signaal gebruikte om dit te testen.}

\subsection{DeepCore}
Additionally a DeepCore specialized filter was added to account for SMP tracks that partially traverse the more densly instrumented DC detector. Due to the low amount of light produced by these dim tracks, adding the DeepCore filter that is specialized for this part of the detector proved to be of significant importance.

The DeepCore filter was designed to look for very dim events coming from, e.g., dark matter, low-energy neutrino oscillations, and studies in observing atmospheric neutrinos below 100 GeV. The fiducial volume used for this filter consists of
\vspace{2mm}
\begin{itemize}
\item the bottom 22 DOMs on the IceCube strings 25, 26, 27, 34, 35, 36, 37, 44, 45, 46, 47 and 54;
\item the bottom 50 DOMs on the DeepCore strings 79-86.
\end{itemize}
\vspace{2mm}
These strings are indicated in Fig. \ref{fig:deepcorestrings}.\\

\begin{figure}[t]
\centering
\includegraphics[width=0.55\textwidth]{chapter8/img/stringview.jpg}
\caption{Aerial view of the IceCube strings (and IceTop tanks) where the DeepCore fiducial volume is defined by the DeepCore strings (red) and several surrounding in-ice IceCube strings (green and red).}
\label{fig:deepcorestrings}
\end{figure}

\noindent The filter uses the DeepCore SMT3 trigger and calculates the COG position. Two layers are used as a veto to remove events that probably originate from atmospheric muons. More information can be found in Ref. \cite{DeepCore2012}.

\subsection{Burnsample checks}
Before further processing, the burn sample (Section \ref{sec:burnsample}) is compared over the different years that are used in the analysis. This is shown in Figure \ref{fig:burnsamplechecks}. More information on the burn sample can be found in Section \ref{sec:burnsample}.

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{chapter8/img/FilterRatePerRun.png}
\includegraphics[width=0.49\textwidth]{chapter8/img/FilterRatePerMonth.png}
\caption{\textit{Left: }Total rate of the combined filters in function of the run number. The sine wave pattern from seasonal variations in the atmosphere (see Section \ref{subsub:corsika}) is clearly visible and consisten throughout the years. The x-axis is more spread out in the first years as there were more test runs. The shift in data rate in early 2011 runs is due to the DOM software change that was introduced in the Summer of 2011 \cite{2011rate}. This phenomenon is well understood and since the changes are minimal it was chosen to keep these runs. \textit{Right: }Total filter rate averaged per month. There is an overlap for each year because a new season doesn't necessarily start in the beginning of a month.}
\label{fig:burnsamplechecks}
\end{figure}

\section{Level 3}
The combined filter selection leads to a total rate of $\sim60$ Hz, or $\sim1.9$ billion events per year. The average event size at Level2 is around 15 kB, which would result into around 30 TB of data per year.

Therefore, five quality cuts are implemented with a goal that is threefold:
\vspace{2mm}
\begin{enumerate}
\item reduce the total rate of the data,
\item improve the signal to background ratio, getting rid of uninteresting events,
\item improve the agreement between data and Monte Carlo.
\end{enumerate}
\vspace{2mm}
\noindent These cuts are shown in Fig. \ref{fig:level3cuts}.

\subsection{Zenith angle cut}
Even though there are no up-going muons from air showers expected, the vast majority of events that pass the filter selection remain from misreconstructed muons. Even though there is only a small chance of these events to have a large misreconstructed zenith angle. The expected flux of air showers is so much larger compared to the assumed signal flux to such an extent that is dominates with orders of magnitude. The majority still has an reconstructed zenith angle lower than 90$^\circ$. Therefore the zenith angle cut was set at an angle of 

\begin{equation}
\theta_\textrm{zen} (\textrm{MPE}) \geq 85^\circ.
\end{equation}

\begin{figure}[ht]
\centering
\includegraphics[width=0.49\textwidth]{chapter8/img/1D_stack_mpefit_zenith.png}
\includegraphics[width=0.49\textwidth]{chapter8/img/1D_stack_mpefit_rlogl.png}
\includegraphics[width=0.49\textwidth]{chapter8/img/L3_zenithcut_gr_1p4835298642_rloglcut_less_15_1D_stack_npe.png}
\includegraphics[width=0.49\textwidth]{chapter8/img/L3_zenithcut_gr_1p4835298642_rloglcut_less_15_npecut_less_50_1D_stack_finitereco_rllh_starting.png}
\includegraphics[width=0.49\textwidth]{chapter8/img/L3_zenithcut_gr_1p4835298642_rloglcut_less_15_npecut_less_50_startingtrackcut_hs_gr_0_1D_stack_finitereco_rllh_stopping.png}
\caption{\textit{First row, left: }Number of events in function of MPE reconstructed zenith angle normalized to the burn sample. The upwards trend to higher zenith angles is due to the filter selections that depend on the angle. \textit{First row, right: }Number of events in function of rlogL normalized to the burn sample. \textit{Second row, left: }Number of events in function of number of photoelectrons (NPE) seen in the detector. \textit{Second row, right: }Number of events in function of the starting likelihood. \textit{Third row: }Number of events in function of the stopping likelihood. The cuts are illustrated with an orange line, the arrow points towards the events that are kept.}
\label{fig:level3cuts}
\end{figure}


\subsection{RlogL cut}
The reduced log-likelihood, rlogL of the track reconstruction fit is used as a goodness-of-fit variable. The term ``reduced'' is used because the logarithm of the likelihood is normalized by the number of degrees of freedom (NDOF) in the track fit

\begin{equation}
\textrm{rlogL} = \frac{\log \mathcal{L}}{\textrm{NDOF}} = \frac{\log \mathcal{L}}{\textrm{NCh} - \textrm{NPara}},
\end{equation}
where NCh is the number of channels/DOMs and NPara the number of fitted parameters (3 for the position and 2 for the track). For Gaussian probability distributions this expression corresponds to the reduced chi-square. Lower values indicate better reconstructions, therefore the rlogL cut was set at a value of 

\begin{equation}
\textrm{rlogL} < 15.
\end{equation}

\subsection{NPE cut}
The number of photoelectrons seen in the detector has a clear correlation to the number of photons that were emitted from the track. From Eq. \ref{eq:franktamm} it is clear that particles with a charge $< 1$ will produce less light. Therefore a cut on the total number of photoelectrons was set at a value of 

\begin{equation}
\textrm{NPE} < 50.
\end{equation} 


\subsection{Starting rlogL cut}
The relative probability for tracks to be starting and/or stopping can be computed with FiniteReco (see Section \ref{subsec:finitereco}). Because most low-energetic muons would be starting and/or stopping in the detector, these likelihoods prove to be a powerful tool in removing these events \footnote{High energetic muons will have a higher change of being throughgoing, but would produce much more light than the dim tracks that are expected for the SMPs.}. The llh is always compared to the llh of throughgoing tracks, hence the ``relative probability''. It was chosen to place the starting rlogL at a value of 

\begin{equation}
\textrm{rlogL} = \textrm{rlogL}(\textrm{starting}) - \textrm{rlogL}(\textrm{throughgoing}) > 0. 
\end{equation}

\subsection{Stopping rlogL cut}
Analogous to the previous cut, it was chosen to place the stopping rlogL at a value of
\begin{equation}
\textrm{rlogL} = \textrm{rlogL}(\textrm{stopping}) - \textrm{rlogL}(\textrm{throughgoing}) > 10. 
\end{equation}


\section{Level 4}
As can be seen in Fig. \ref{fig:level3cuts}, most of the background still originates from air showers (referred to as CORSIKA). Due to the Level 3 quality cuts, the total rate was reduced from around $\sim 60$ Hz to $\sim2$ Hz, low enough for more elaborate variables to be computed and more elaborate cleaning. In Level 4 I have implemented the IceHive splitting and cleaning tools (see Section \ref{sec:icehive}) and rerun the particle reconstructions on these ``new'' events. Additional quality cuts were added to this level to ensure higher quality events. An overview is given in Table \ref{tab:level4}. Finally, new variables were constructed to use in Level 5.

\begin{table}[]
\caption{Overview of quality cuts in Level 4.}
\label{tab:level4}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{9B9B9B}}l |c|c|c|}
\hline
Variable & \cellcolor[HTML]{9B9B9B}Definition & \cellcolor[HTML]{9B9B9B}Cut & \cellcolor[HTML]{9B9B9B}Motivation \\ \hline
nCh & Number of hit DOMs & $\geq 5$ & Allows for better reconstructions \\ \hline
nStr & Number of hit strings & $\geq 2$ & Allows for better reconstructions \\ \hline
nStr\_in & \begin{tabular}[c]{@{}c@{}}The number of hit inner strings.\\ An inner string is not located at the edge of the detector\end{tabular} & $\geq 1$ & Reduce leak-in events \\ \hline
Fitstatus MPE & Status of MPE reconstruction & Status == 'OK' & Remove bad reconstructions \\ \hline
$\theta_{HC}$ (MPE) & Zenith angle cut on HiveCleaned pulses & $\geq 85^\circ$ & \begin{tabular}[c]{@{}c@{}}Similar to cut explained in ???:\\ focus on up-going tracks\end{tabular} \\ \hline
Innerstring domination & See text inline & == True & See text inline \\ \hline
\end{tabular}%
}
\end{table}

\subsection{Cleaning and quality cuts}
\noindent IceHive provides for a thourough cleaning method, sometimes resulting into events with a very low amount of hit DOMs. However, a minimal amount of hits is required to have reasonable and trustworthy particle reconstructions. Similarly, more than one string should have a hit to allow for better reconstructions due to the sparse distribution of the strings in the detector. Because light is able to reach the edge of the detector, even if the closest approach of the particle is tens or hunderds of meters away for very bright events, is would be near impossible to distinguish bright events far from the detector to dim tracks passing close by. Therefore, it was required that at least one string not on the edge of the detector should have hit DOMs to reduce these \textit{leak-in events}.
The zenith angle cut is re-introduced on the new event that should have better reconstructions due to cleaning and finally there is a requirement for ``innerstring domination''.

\paragraph{Innerstring domination}
There persist classes of events at the boundary of the detector, which can be a problem for an upgoing track analysis. This includes event classes as:

\vspace{2mm}
\begin{itemize}
\item (Leak in) Events which are heading towards the instrumented volume, but stop right before they reach it or pass next to, but not to far from, the detector. These leak light to the detector boundaries.
\item (Boundary) Events that penetrate the detector very shallow on the boundary lines and possibly have a cascade at the endpoint. The events have rather cascade like characteristics.
\item (Corner-clippers) Events that are throughgoing on the corners of the detector that have a COG at a corner of the detector.
\item (Leak out) Events originating from a neutrino passing through almost the entire length of the detector and only have an interaction vertex right before leaving the detector. Depending on position and angle, the reverse direction of reconstruction can be of similar probability and thus a nuisance.
\end{itemize}
\vspace{2mm}

\noindent All these event classes are not well reconstructable or have a high uncertainty in the reconstruction. It is more feasible to remove these class of events to maintain a sample of well reconstructable events. This is done here by the requirement of innerstring domination.

DOMs are defined as outer DOMs if they are one of the following:

\vspace{2mm}
\begin{itemize}
\item part of a string on edge of the detector,
\item on the bottom of strings 1-78,
\item on the top of strings 1-78.
\end{itemize}
\vspace{2mm}

\noindent The innerstring domination is set to \texttt{True} when 

\begin{equation}
\frac{\# \textrm{outer DOMs}}{\#\textrm{inner DOMs}} < 0.5.
\end{equation}

\subsection{Variable construction}
To distinguish signal from background events, variables that show a clear distribution difference prove to have the most discriminative power. In this part of the analysis, multiple new variables are introduced with this goal. Some variables used in Level 5 are already explained in the text and need no further introduction. A summary is given in Table \ref{tab:allvariables}.

\subsubsection{Commonvariables}
Variables that were often used in analyses often had subtle differences between them, making them prone to be a cause of errors. Multiple variable were therefore combined into one project, called ``Commonvariables''. The variables used here can be subdivided into three categories: track characteristics, hit statistics and time characteristics and are summarized in Table \ref{tab:commonvariables}.

\begin{table}[]
\centering
\caption{List of Commonvariables used in this analysis.} 
%\vspace{2mm}
\begin{flushleft}
\begin{footnotesize}
$^\dagger$ Whenever one of the track characteristics variables is shown/mentioned, the suffix (e.g. \_50) refers to the track cylinder that was used around the track.
\end{footnotesize}
\end{flushleft}
\label{tab:commonvariables}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{9B9B9B}}l |c|c|}
\hline
Category & \cellcolor[HTML]{9B9B9B}Variable & \cellcolor[HTML]{9B9B9B}Description \\ \hline
\begin{tabular}[c]{@{}l@{}}Track\\ Characteristics$^\dagger$\end{tabular} & AvgDistToDom & \begin{tabular}[c]{@{}c@{}}The average distance of the DOMs to the reconstructed track, \\ weighted by the total charge of each DOM.\end{tabular} \\ \cline{2-3} 
 & EmptyHits & \begin{tabular}[c]{@{}c@{}}The maximal track length along the reconstructed track that \\ got no hits within a cylinder around the track.\end{tabular} \\ \cline{2-3} 
 & TrackSeparation & \begin{tabular}[c]{@{}c@{}}Distance how far the COG positions of the first and the \\ last quartile of the hits are separated from each other.\end{tabular} \\ \cline{2-3} 
 & TrackDistribution & \begin{tabular}[c]{@{}c@{}}The track hits distribution smoothness value {[}-1;1{]} \\ shows how smooth the hits of the given pulse series\\ within the specified track cylinder radius are distributed \\ along the track.\end{tabular} \\ \hline
Hit Statistics & ZTravel & \begin{tabular}[c]{@{}c@{}}Z value of first quartile (in time) of the hit DOMs is calculated.\\ ZTravel is the average difference of the z value of all hit DOMs\\ with the first quartile z value.\end{tabular} \\ \cline{2-3} 
 & ZMax & The maximum z of all hit DOMs. \\ \hline
\begin{tabular}[c]{@{}l@{}}Time\\ Characteristics\end{tabular} & ZPattern & \begin{tabular}[c]{@{}c@{}}All first pulses per DOM are ordered in time. If a DOM\\ position of a pulse is higher than the previous ZPattern\\ is increased with +1. If the second pulse is located lower \\ in the detector ZPattern decreases with -1. \\ In general this variable gives a tendency of the direction of a track.\end{tabular} \\ \hline
\end{tabular}%
}
\end{table}
Because DC and IC DOMs have different quantum efficiencies (see Section \ref{subsec:DC}), the pulses from DC and IC DOMs should not be mixed for an unambiguous definition. Therefore either only DC or IC pulses are used to compute these variables depending on if an event is \textit{IC dominated} or \textit{DC dominated}, where the former is set at $\frac{\# \textrm{DOMs}_\textrm{IC}}{\# \textrm{DOMs}_\textrm{DC}} >= 0.5$ and the latter otherwise.

\subsubsection{Millipede variables}
The \texttt{Millipede} toolkit was introduced in Section \ref{subsec:millipede}, where it was explained how the energy deposition could be estimated from the light seen by the individual DOMs. Constructing multiple variables from this toolkit was the master thesis subject of Stef Verpoest and can be found in Ref. \cite{steffthesis} for an elaborate explanation. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{chapter8/img/millipedeStef.png}
\caption{Output of a \texttt{Millipede} fit for an SMP with charge $\frac{1}{3}$ and mass 10 GeV. The x-axis shows the distance the particle traveled and starts after the first simulated energy loss event. The fit tries to estimate the energy of 15 m track segments. As a comparison, the true positions of energy deposits from the MC simulation are shown in green. Locations outside the detector are shaded in grey.}
\label{fig:millipedeoutput}
\end{figure}

\subsubsection{New variables}

\begin{table}[]
\footnotesize

\caption{My caption}
\label{my-label}
\begin{tabular}{|l|r|c|c|}
\hline
\rowcolor[HTML]{9B9B9B} 
Class & \multicolumn{1}{|c|}{Variable} & \begin{tabular}[c]{@{}c@{}}MRMR\\score\end{tabular} & \multicolumn{1}{l|}{\cellcolor[HTML]{9B9B9B}Importance} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{9B9B9B}} & ZMax & 2 & 0.109 \\ \cline{2-4} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{9B9B9B}} & ZTravel & 3 & 0.106 \\ \cline{2-4} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{9B9B9B}} & AvgDistToDom\_150 & 9 & 0.048 \\ \cline{2-4} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{9B9B9B}} & TrackSeparation\_150 & 10 & 0.043 \\ \cline{2-4} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{9B9B9B}} & TrackDistribution\_50 & 12 & 0.035 \\ \cline{2-4} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{9B9B9B}} & TrackSeparation\_50 & 13 & 0.034 \\ \cline{2-4} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{9B9B9B}} & EmptyHits\_100 & 16 & 0.027 \\ \cline{2-4} 
\multicolumn{1}{|c|}{\multirow{-8}{*}{\cellcolor[HTML]{9B9B9B}Commonvariables}} & ZPattern & 17 & 0.016 \\ \hline
\cellcolor[HTML]{9B9B9B} & RunsAboveMean & 4 & 0.105 \\ \cline{2-4} 
\cellcolor[HTML]{9B9B9B} & Mean\_dEdX & 5 & 0.074 \\ \cline{2-4} 
\multirow{-3}{*}{\cellcolor[HTML]{9B9B9B}Millipede} & TrackLength\_60 & 11 & 0.039 \\ \hline
\cellcolor[HTML]{9B9B9B} & NewLength\_150 & 1 & 0.132 \\ \cline{2-4} 
\cellcolor[HTML]{9B9B9B} & NewLength\_all\_z & 6 & 0.059 \\ \cline{2-4} 
\multirow{-3}{*}{\cellcolor[HTML]{9B9B9B}New variables} & SpeedRatio & 14 & 0.033 \\ \hline
\cellcolor[HTML]{9B9B9B} & LineFit\_Velocity & 7 & 0.055 \\ \cline{2-4} 
\cellcolor[HTML]{9B9B9B} & $\sigma_{\textrm{para}}$ & 8 & 0.051 \\ \cline{2-4} 
\multirow{-3}{*}{\cellcolor[HTML]{9B9B9B}Other variables} & $\cos(\theta)_{\textrm{SPE}}$ & 15 & 0.033 \\ \hline
\end{tabular}
\begin{tabular}{|c|r|}
\hline
\rowcolor[HTML]{9B9B9B} 
\begin{tabular}[c]{@{}c@{}}MRMR\\score\end{tabular}& Variable \\ \hline
1 & NewLength\_150 \\ \hline
2 & ZMax \\ \hline
3 & ZTravel \\ \hline
4 & RunsAboveMean \\ \hline
5 & Mean\_dEdX \\ \hline
6 & NewLength\_all\_z \\ \hline
7 & LineFit\_Velocity \\ \hline
8 & $\sigma_\textrm{para}$ \\ \hline
9 & AvgDistToDom\_150 \\ \hline
10 & TrackSeparation\_150 \\ \hline
11 & TrackLength\_60 \\ \hline
12 & TrackDistribution\_50 \\ \hline
13 & TrackSeparation\_50 \\ \hline
14 & SpeedRatio \\ \hline
15 & $\cos(\theta)_\textrm{SPE}$ \\ \hline
16 & EmptyHits\_100 \\ \hline
17 & ZPattern \\ \hline
\end{tabular}
\end{table}

\section{Level 5}
\subsection{MRMR}
\subsection{BDT}

\url{https://arxiv.org/pdf/physics/0312102v1.pdf}\\
\url{https://arxiv.org/pdf/1310.1284.pdf}

Ook ergens een tabel maken met info over je data runs. Duidelijk maken wat de livetime is bv en ook zeggen van wanneer to wanneer een bepaalde run liep (2011: mei 2011- mei 2012)

Klaus zijn paper? \url{https://arxiv.org/abs/1806.05696}


\section{Pull validation}
\section{Systematic Uncertainties}

\section{Results}